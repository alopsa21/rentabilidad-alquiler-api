# üìä Configuraci√≥n del LLM y Control de Costos

Este documento explica c√≥mo controlar los costos de OpenAI ajustando la configuraci√≥n del servicio LLM.

## üìç Ubicaci√≥n de la Configuraci√≥n

**TODO se configura en un solo archivo:**

```
src/config/llm.config.ts
```

## üí° Input vs Output Tokens

Es crucial entender la diferencia:

### Input (Prompt) - LO QUE ENVIAMOS
- El texto del anuncio scrapeado
- En nuestro caso: ~500 tokens por llamada
- **Costo:** ~$0.15 por 1M tokens (m√°s barato)
- **NO se controla con `maxTokens`**

### Output (Respuesta) - LO QUE RECIBIMOS
- El JSON con los datos extra√≠dos
- En nuestro caso: ~80-120 tokens por llamada
- **Costo:** ~$0.60 por 1M tokens (4x m√°s caro)
- **S√ç se controla con `maxTokens`** ‚≠ê

**Conclusi√≥n:** Aunque el output es menor en tokens, es 4x m√°s caro por token. Por eso `maxTokens` es importante para controlar costos.

---

## üéõÔ∏è Configuraciones Disponibles

### 1Ô∏è‚É£ Configuraci√≥n del Modelo (`LLM_CONFIG`)

```typescript
export const LLM_CONFIG = {
  model: 'gpt-4o-mini',    // Modelo a usar
  maxTokens: 150,          // Tokens m√°ximos de respuesta
  temperature: 0.1,        // Determinismo (0-2)
  timeout: 10000,          // Timeout en ms
}
```

#### `model` - Modelo de OpenAI

**Opciones (de m√°s barato a m√°s caro):**
- `'gpt-4o-mini'` ‚≠ê **RECOMENDADO** - ~$0.15 por 1M tokens
- `'gpt-3.5-turbo'` - ~$0.50 por 1M tokens  
- `'gpt-4o'` - ~$5 por 1M tokens

**Para ahorrar:** Usa `gpt-4o-mini` (actual).

---

#### `maxTokens` - L√≠mite de respuesta (OUTPUT)

‚ö†Ô∏è **IMPORTANTE:** Este l√≠mite es para la **respuesta del LLM (output)**, NO para el prompt (input).

**Contexto:**
- **Prompt (input):** ~500 tokens (texto del anuncio) - NO se limita aqu√≠
- **Respuesta (output):** ~80-120 tokens (nuestro JSON) - S√ç se limita aqu√≠

**¬øQu√© son tokens?**
- 1 token ‚âà 0.75 palabras en espa√±ol
- 100 tokens ‚âà 75 palabras
- 150 tokens ‚âà 112 palabras ‚≠ê **ACTUAL** (suficiente para nuestro JSON)
- 200 tokens ‚âà 150 palabras

**Costos:**
- Input (prompt): ~$0.15 por 1M tokens
- Output (respuesta): ~$0.60 por 1M tokens (4x m√°s caro)

**Para ahorrar:** Reduce a `100` si el JSON actual es demasiado. La respuesta se cortar√° si excede el l√≠mite.

---

#### `temperature` - Creatividad

- `0.0` - Muy determinista
- `0.1` ‚≠ê **ACTUAL** - Casi determinista (ideal para datos)
- `0.7` - Equilibrado
- `2.0` - Muy creativo

**Para ahorrar:** Mant√©n en `0.1` (m√°s consistente = menos tokens desperdiciados).

---

#### `timeout` - Tiempo m√°ximo

- `10000` ‚≠ê **ACTUAL** = 10 segundos
- Reduce a `5000` (5s) si quieres respuestas m√°s r√°pidas
- Aumenta a `15000` (15s) si tienes timeouts frecuentes

---

### 2Ô∏è‚É£ Rate Limiting (`LLM_RATE_LIMIT`)

```typescript
export const LLM_RATE_LIMIT = {
  maxRequestsPerMinute: 10,    // Requests por minuto
  maxCallsPerDay: 1000,        // Llamadas LLM por d√≠a
}
```

#### `maxRequestsPerMinute` - L√≠mite por minuto

Controla cu√°ntas requests puede hacer un usuario por minuto.

**Valores recomendados:**
- `5` - Muy restrictivo
- `10` ‚≠ê **ACTUAL** - Para desarrollo/testing
- `20-50` - Producci√≥n peque√±a
- `100+` - Producci√≥n grande

---

#### `maxCallsPerDay` - L√≠mite diario üî• **IMPORTANTE**

**Esta es tu protecci√≥n principal contra gastos excesivos.**

Cuando se alcanza el l√≠mite, el sistema usa valores fallback (no llama al LLM).

**C√°lculo de costos con `gpt-4o-mini`:**

Por llamada:
- Input (~500 tokens): ~$0.000075
- Output (~100 tokens con maxTokens=150): ~$0.00006
- **Total por llamada: ~$0.000135**

| Llamadas/d√≠a | Costo/d√≠a | Costo/mes |
|-------------|-----------|-----------|
| 100 | $0.01 | $0.30 |
| 500 | $0.07 | $2.00 |
| 1000 ‚≠ê | $0.14 | $4.00 |
| 5000 | $0.68 | $20.00 |
| 10000 | $1.35 | $40.00 |

üí° **Nota:** Los tokens de output son 4x m√°s caros que los de input, por eso `maxTokens` es importante para controlar costos.

**Para ahorrar:**
- Testing: `100`
- MVP: `500`
- Producci√≥n peque√±a: `1000` ‚≠ê **ACTUAL**
- Producci√≥n media: `5000`

**Para deshabilitar:** Establece en `0`.

---

### 3Ô∏è‚É£ Cach√© (`LLM_CACHE_CONFIG`)

```typescript
export const LLM_CACHE_CONFIG = {
  enabled: true,              // Habilitar cach√©
  ttl: 60 * 60 * 1000,       // 1 hora
  maxEntries: 1000,           // M√°ximo de URLs en cach√©
}
```

#### `enabled` - Activar/Desactivar cach√©

- `true` ‚≠ê **RECOMENDADO** - Ahorra muchas llamadas en URLs repetidas
- `false` - Solo para debugging

**Ahorro t√≠pico:** 30-70% de llamadas si hay URLs repetidas.

---

#### `ttl` - Tiempo de vida

Tiempo que una URL permanece en cach√© antes de volver a llamar al LLM.

**Valores comunes:**
- `5 * 60 * 1000` = 5 minutos
- `60 * 60 * 1000` ‚≠ê **ACTUAL** = 1 hora
- `24 * 60 * 60 * 1000` = 24 horas

**Para ahorrar m√°s:** Aumenta a 24 horas si los anuncios cambian poco.

---

#### `maxEntries` - M√°ximo de entradas

N√∫mero m√°ximo de URLs diferentes en cach√©.

**Uso de memoria:**
- 1000 entradas ‚≠ê **ACTUAL** ‚âà 1-2 MB
- 10000 entradas ‚âà 10-20 MB

---

## üéØ Escenarios de Uso

### üí∞ Presupuesto MUY ajustado

```typescript
export const LLM_CONFIG = {
  model: 'gpt-4o-mini',
  maxTokens: 100,        // ‚¨áÔ∏è Reducido
  temperature: 0.1,
  timeout: 10000,
}

export const LLM_RATE_LIMIT = {
  maxRequestsPerMinute: 5,    // ‚¨áÔ∏è M√°s restrictivo
  maxCallsPerDay: 100,        // ‚¨áÔ∏è Muy limitado
}

export const LLM_CACHE_CONFIG = {
  enabled: true,
  ttl: 24 * 60 * 60 * 1000,   // ‚¨ÜÔ∏è 24 horas
  maxEntries: 1000,
}
```

**Costo mensual:** ~$0.50/mes

---

### ‚öñÔ∏è Equilibrio precio/calidad (ACTUAL)

```typescript
export const LLM_CONFIG = {
  model: 'gpt-4o-mini',
  maxTokens: 150,
  temperature: 0.1,
  timeout: 10000,
}

export const LLM_RATE_LIMIT = {
  maxRequestsPerMinute: 10,
  maxCallsPerDay: 1000,
}

export const LLM_CACHE_CONFIG = {
  enabled: true,
  ttl: 60 * 60 * 1000,  // 1 hora
  maxEntries: 1000,
}
```

**Costo mensual:** ~$4/mes (con cach√©: ~$2/mes)

---

### üöÄ Producci√≥n con volumen

```typescript
export const LLM_CONFIG = {
  model: 'gpt-4o-mini',
  maxTokens: 200,        // ‚¨ÜÔ∏è M√°s margen
  temperature: 0.1,
  timeout: 10000,
}

export const LLM_RATE_LIMIT = {
  maxRequestsPerMinute: 50,   // ‚¨ÜÔ∏è M√°s permisivo
  maxCallsPerDay: 5000,       // ‚¨ÜÔ∏è Mayor volumen
}

export const LLM_CACHE_CONFIG = {
  enabled: true,
  ttl: 60 * 60 * 1000,
  maxEntries: 10000,    // ‚¨ÜÔ∏è M√°s cach√©
}
```

**Costo mensual:** ~$20/mes (con cach√©: ~$10/mes)

---

## üìä Monitoreo de Costos

Los logs muestran informaci√≥n √∫til:

```bash
# Llamada exitosa
{"callCount":42,"limit":1000,"remaining":958,"percentage":"4%"}

# Advertencia cerca del l√≠mite
‚ö†Ô∏è  Acerc√°ndose al l√≠mite diario de llamadas LLM (remaining: 50)

# L√≠mite alcanzado
üö® L√≠mite diario de llamadas LLM alcanzado. Usando valores fallback.

# Cache hit (ahorro)
Cache hit (ahorro de llamada LLM) age: 145s
```

---

## ‚úÖ Recomendaciones

1. **Empieza conservador**: `maxCallsPerDay: 100` en testing
2. **Monitorea los logs**: Ajusta seg√∫n uso real
3. **Activa el cach√©**: Ahorra 30-70% f√°cilmente
4. **Mant√©n `gpt-4o-mini`**: Es suficiente y 15x m√°s barato que GPT-4
5. **Reduce `maxTokens`**: Si no necesitas respuestas largas

---

## üîÑ C√≥mo Cambiar la Configuraci√≥n

1. Edita `src/config/llm.config.ts`
2. Cambia los valores que necesites
3. Reinicia el servidor: `npm run dev`
4. Los cambios son inmediatos (no requiere recompilaci√≥n)

---

## üìù Variables de Entorno Relacionadas

```bash
# .env
OPENAI_API_KEY=sk-...    # Tu API key de OpenAI
```

**NO cambies esto en el c√≥digo**, usa variables de entorno.
